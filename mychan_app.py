import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
import xgboost as xgb
from sklearn.metrics import (confusion_matrix, ConfusionMatrixDisplay,
                             classification_report, roc_curve, auc)

st.set_page_config(page_title="ì‚°ë¶ˆ ì˜ˆì¸¡ í¬ë™ AI ëŒ€ì‹œë³´ë“œ", layout='wide')

st.title("ğŸ”¥ í™˜ê²½ì„ ì§€í‚¤ëŠ”  í¬ë™ AI - ì‚°ë¶ˆ ì˜ˆì¸¡ ëŒ€ì‹œë³´ë“œ")

# 1. ë°ì´í„° ì—…ë¡œë“œ & ë¯¸ë¦¬ë³´ê¸°
st.header("1ï¸. ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°")
uploaded_file = st.file_uploader("fire.csv.xlsx íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type=["xlsx"])
if uploaded_file:
    df = pd.read_excel(uploaded_file)
    st.write("ë°ì´í„° ìƒ˜í”Œ", df.head())

    # ë³€ìˆ˜ ì„ íƒ
    numeric_cols = ['top_tprt', 'avg_hmd', 'ave_wdsp', 'de_rnfl_qy']
    target_col = 'frfire_ocrn_nt'

    # ê²°ì¸¡ì¹˜ ì²˜ë¦¬
    df = df.dropna(subset=[target_col])
    for col in numeric_cols:
        df[col] = df[col].fillna(df[col].mean())

    # íƒ€ê²Ÿ ì´ì§„í™”
    df[target_col] = (df[target_col] > 0).astype(int)

    st.subheader("íƒ€ê²Ÿ(ì‚°ë¶ˆ ë°œìƒ) ë¶„í¬")
    st.bar_chart(df[target_col].value_counts())

    # 2. ë³€ìˆ˜ë³„ íˆìŠ¤í† ê·¸ë¨
    st.header("2ï¸. ì£¼ìš” ë³€ìˆ˜ë³„ ë¶„í¬")
    fig, axs = plt.subplots(2, 2, figsize=(12, 6))
    for i, col in enumerate(numeric_cols):
        ax = axs[i//2, i%2]
        sns.histplot(df[col], bins=20, kde=True, ax=ax)
        ax.set_title(col)
    st.pyplot(fig)

    # 3. ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ
    st.header("3ï¸. ì£¼ìš” ë³€ìˆ˜ ê°„ ìƒê´€ê´€ê³„")
    corr_matrix = df[numeric_cols].corr()
    fig_corr, ax_corr = plt.subplots()
    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', linewidths=0.5, fmt=".2f", ax=ax_corr)
    st.pyplot(fig_corr)

    # 4. ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
    st.header("4ï¸. ì‚°ë¶ˆ ì˜ˆì¸¡ AI ëª¨ë¸ ë¹„êµ")

    features = numeric_cols
    X = df[features]
    y = df[target_col]

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # ëª¨ë¸ ì„ ì–¸/í•™ìŠµ
    rf = RandomForestClassifier(n_estimators=100, random_state=42)
    rf.fit(X_train, y_train)
    y_pred_rf = rf.predict(X_test)

    lr = LogisticRegression(max_iter=1000)
    lr.fit(X_train, y_train)
    y_pred_lr = lr.predict(X_test)

    xgb_model = xgb.XGBClassifier(n_estimators=100, use_label_encoder=False, eval_metric='logloss')
    xgb_model.fit(X_train, y_train)
    y_pred_xgb = xgb_model.predict(X_test)

    # í˜¼ë™í–‰ë ¬ í•¨ìˆ˜
    def plot_confusion(y_true, y_pred, title):
        labels = np.unique([*y_true, *y_pred])
        cm = confusion_matrix(y_true, y_pred, labels=labels)
        fig_cm, ax_cm = plt.subplots()
        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)
        disp.plot(cmap='Blues', ax=ax_cm)
        ax_cm.set_title(title)
        st.pyplot(fig_cm)

    # ëª¨ë¸ë³„ í˜¼ë™í–‰ë ¬ ë° ì„±ëŠ¥ì§€í‘œ
    st.subheader("ëª¨ë¸ë³„ í˜¼ë™í–‰ë ¬ ë° í‰ê°€")
    col1, col2, col3 = st.columns(3)
    with col1:
        st.markdown("**RandomForest**")
        plot_confusion(y_test, y_pred_rf, "RandomForest í˜¼ë™í–‰ë ¬")
        st.text(classification_report(y_test, y_pred_rf))
    with col2:
        st.markdown("**LogisticRegression**")
        plot_confusion(y_test, y_pred_lr, "LogisticRegression í˜¼ë™í–‰ë ¬")
        st.text(classification_report(y_test, y_pred_lr))
    with col3:
        st.markdown("**XGBoost**")
        plot_confusion(y_test, y_pred_xgb, "XGBoost í˜¼ë™í–‰ë ¬")
        st.text(classification_report(y_test, y_pred_xgb))

    # ROC Curve ë¹„êµ
    st.subheader("ëª¨ë¸ë³„ ROC Curve & AUC")
    fig_roc, ax_roc = plt.subplots(figsize=(8,6))

    def plot_roc(model, X, y, label):
        y_proba = model.predict_proba(X)[:,1]
        fpr, tpr, _ = roc_curve(y, y_proba)
        auc_score = auc(fpr, tpr)
        ax_roc.plot(fpr, tpr, label=f"{label} (AUC={auc_score:.2f})")
        return auc_score

    auc_rf = plot_roc(rf, X_test, y_test, "RandomForest")
    auc_lr = plot_roc(lr, X_test, y_test, "LogisticRegression")
    auc_xgb = plot_roc(xgb_model, X_test, y_test, "XGBoost")

    ax_roc.plot([0,1],[0,1],'k--',label='Random (AUC=0.5)')
    ax_roc.set_xlabel('False Positive Rate')
    ax_roc.set_ylabel('True Positive Rate')
    ax_roc.set_title('ëª¨ë¸ë³„ ROC Curve ë° AUC')
    ax_roc.legend()
    ax_roc.grid(True)
    st.pyplot(fig_roc)

    # ë³€ìˆ˜ ì¤‘ìš”ë„
    st.header("5ï¸. ëœë¤í¬ë ˆìŠ¤íŠ¸ ë³€ìˆ˜ ì¤‘ìš”ë„")
    importances = rf.feature_importances_
    indices = np.argsort(importances)[::-1]
    fig_imp, ax_imp = plt.subplots()
    ax_imp.bar(range(X.shape[1]), importances[indices], align='center')
    ax_imp.set_xticks(range(X.shape[1]))
    ax_imp.set_xticklabels(np.array(X.columns)[indices], rotation=45)
    ax_imp.set_title("Feature Importances (RandomForest)")
    st.pyplot(fig_imp)

    st.markdown("---")
    st.success("AI ê¸°ë°˜ ì‚°ë¶ˆ ì˜ˆì¸¡ ê²°ê³¼ì™€ ë°ì´í„° EDAë¥¼ í™•ì¸í–ˆìŠµë‹ˆë‹¤! \n(ì‹¤ì œ ì •ì±…ì  í™œìš© ì‹œ, FN(ì‹¤ì œ ë°œìƒì¸ë° ë¯¸ë°œìƒìœ¼ë¡œ ì˜ˆì¸¡í•œ ê²½ìš°) ê°ì†Œ ì „ëµì— ì£¼ëª©í•˜ì„¸ìš”.)")
else:
    st.info("ë¨¼ì € ë°ì´í„°ë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”.")
